{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM et CNN pour la classification de textes\n",
    "## Analyse de sentiments appliquée à des tweets en français\n",
    "\n",
    "Issu du concours académique DEFT'2017<br/>\n",
    "https://deft.limsi.fr/2017/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "\n",
    "pylab.rcParams['figure.figsize'] = (15.0, 10.0)\n",
    "pylab.rcParams['figure.dpi'] = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"max_colwidth\", 999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement de la donnée\n",
    "On prendra comme jeu de données celui concernant les tweets figuratifs. Il s'agit ici de détecter si un tweet utilise du langage figuratif ou non. On s'intéresse à trois phénomènes : l'ironie, le sarcasme et l'humour. Si un tweet contient au moins une expression relevant de ces phénomènes, il est considéré comme figuratif, sinon il est considéré non figuratif."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = pd.read_csv('../tests_data/deft2017/task2-train.csv',skiprows=10, sep='\\t', header=None, encoding='utf-8', quoting=3)\n",
    "X.columns = ['id','text','class']\n",
    "X = X.set_index('id')\n",
    "X = X.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Train de vie des ministères : Fillon conteste les chiffres http://tinyurl.com/y89zgkk Qui ment? ()</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Un complot sioniste pour augmenter les accidents mortels en Tunisie http://snipr.com/ua2dw</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[rp-fr][Numerama] La Chine rejoint l'Inde pour s'opposer à l'ACTA http://ur1.ca/05rxl</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LeGvnt maitrise la situation RT @cedricgarrofe Incendies en Russie: état d'urgence autour d'un centre nucléaire http://bit.ly/doqgSq</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Chanceux! RT @lemondefr Le patron de Google reçu par Nicolas Sarkozy http://bit.ly/aYrNCx</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                   text  \\\n",
       "0                                   Train de vie des ministères : Fillon conteste les chiffres http://tinyurl.com/y89zgkk Qui ment? ()    \n",
       "1                                            Un complot sioniste pour augmenter les accidents mortels en Tunisie http://snipr.com/ua2dw   \n",
       "2                                                 [rp-fr][Numerama] La Chine rejoint l'Inde pour s'opposer à l'ACTA http://ur1.ca/05rxl   \n",
       "3  LeGvnt maitrise la situation RT @cedricgarrofe Incendies en Russie: état d'urgence autour d'un centre nucléaire http://bit.ly/doqgSq   \n",
       "4                                             Chanceux! RT @lemondefr Le patron de Google reçu par Nicolas Sarkozy http://bit.ly/aYrNCx   \n",
       "\n",
       "        class  \n",
       "0  figurative  \n",
       "1  figurative  \n",
       "2  figurative  \n",
       "3  figurative  \n",
       "4  figurative  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un peu de remise en forme de la donnée (cible, netoyage):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X['class_cat'] = X['class'].map(lambda x : 1 if x == 'figurative' else 0).astype(\"str\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# X['text'] = X['text'].str.encode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def parse_tweet(text):\n",
    "    text = text.replace('@', '')\n",
    "    text = text.replace('#', '')\n",
    "    text = text.replace('\\n', '')\n",
    "    text = text.replace('\\r','')\n",
    "    \n",
    "    urls = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', text)\n",
    "    for url in urls:\n",
    "        text = text.replace(url,'url')\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On nettoie rapidement le texte, et on remplace tous les URLs par un même mot clé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X['text'] = X['text'].astype(\"str\").map(lambda x : parse_tweet(x))\n",
    "X['text'] = X['text'].map(lambda x : parse_tweet(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def remove_punct(text):\n",
    "    for p in string.punctuation:\n",
    "        text = text.replace(p, ' ')\n",
    "    text = ' '.join(text.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On retire la ponctuation et on passe tout en minuscule. Ce n'est pas forcément une bonne idée d'enlever toute la ponctuation dans le cas d'un problème d'analyse de sentiment, mais cela fera l'affaire pour ce tutoriel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['text'] = X['text'].map(lambda x: remove_punct(x))\n",
    "X['text'] = X['text'].map(lambda x: x.lower())\n",
    "# X['text'] = X['text'].str.decode('utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "      <th>class_cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train de vie des ministères fillon conteste les chiffres url qui ment</td>\n",
       "      <td>figurative</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>un complot sioniste pour augmenter les accidents mortels en tunisie url</td>\n",
       "      <td>figurative</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rp fr numerama la chine rejoint l inde pour s opposer à l acta url</td>\n",
       "      <td>figurative</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>legvnt maitrise la situation rt cedricgarrofe incendies en russie état d urgence autour d un centre nucléaire url</td>\n",
       "      <td>figurative</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>chanceux rt lemondefr le patron de google reçu par nicolas sarkozy url</td>\n",
       "      <td>figurative</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                text  \\\n",
       "0                                              train de vie des ministères fillon conteste les chiffres url qui ment   \n",
       "1                                            un complot sioniste pour augmenter les accidents mortels en tunisie url   \n",
       "2                                                 rp fr numerama la chine rejoint l inde pour s opposer à l acta url   \n",
       "3  legvnt maitrise la situation rt cedricgarrofe incendies en russie état d urgence autour d un centre nucléaire url   \n",
       "4                                             chanceux rt lemondefr le patron de google reçu par nicolas sarkozy url   \n",
       "\n",
       "        class class_cat  \n",
       "0  figurative         1  \n",
       "1  figurative         1  \n",
       "2  figurative         1  \n",
       "3  figurative         1  \n",
       "4  figurative         1  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On construit nos jeux de train et test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X['text'], X['class_cat'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(72,)\n",
      "(18,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text2Sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une fois le texte nettoyé, on va effetuer plusieurs étapes pour transformer la donnée et la passer en entrée au réseau de neurones:\n",
    "<ul> \n",
    "<li>**Tokenization**: `'comment est votre blanquette'` devient `['comment', 'est', 'votre', 'blanquette']`</li>\n",
    "<li>**Stemming**: pour réduire le vocabulaire, on ne garde que la racine des mots. `['comment', 'est', 'votr', 'blanquet']`</li>\n",
    "<li>**Dictonnaire de corpus**: on stocke tous les tokens stemmés dans un dictionnaire.</li>\n",
    "<li>**Token2Id**: chaque mot du corpus possède un id unique, et chaque séquence de mots est transformée en une séquence d'ids. `['comment', 'est', 'votr', 'blanquet']` devient `[14, 3, 36, 23]`</li>\n",
    "<li>**Normalisation de la taille des séquences**: un réseau de neurones prend une taille d'entrée, il faut donc normaliser la taille des séquences. Arbitrairement, on choisit la taille moyenne des séquences + 2 fois l'écart-type. Les séquences trop grandes seront tronquées et les séquences trop petites seront *paddées*. Si `0` est la valeur de padding, avec 10 comme taille fixe de séquence, `[14, 3, 36, 23]` devient `[0, 0, 0, 0, 0, 0, 14, 3, 36, 23]`</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Réseau de neurones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import FrenchStemmer\n",
    "from bdacore.neural_networks.processing import Text2Sequence\n",
    "\n",
    "from bdacore.neural_networks.keras_factory import LSTMFactory, CNN_LSTMFactory\n",
    "from bdacore.neural_networks.processing.workflow import TextNeuralNetPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La factory LSTMFactory renvoie un réseau de neurones avec cette architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lstm_model = LSTMFactory().create_model(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La factory CNN_LSTMFactory renvoie un réseau de neurones avec cette architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnn_lstm_model = CNN_LSTMFactory().create_model(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On créé le pipeline avec la factory désirée, et la cardinalité de la cible. Dans notre cas, nous utiliserons la factory la plus simple, soit la LSTMFactory. La classe `TextNeuralNetPipeline` permet d'effectuer le pré-traitement du texte en séquences d'entiers, ainsi que fit/predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = TextNeuralNetPipeline(text2seq=Text2Sequence(stemmer=FrenchStemmer()), factory_class=LSTMFactory, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "72/72 [==============================] - 1s 14ms/step - loss: 0.6931 - acc: 0.5000\n",
      "Epoch 2/5\n",
      "72/72 [==============================] - 0s 502us/step - loss: 0.6861 - acc: 0.6944\n",
      "Epoch 3/5\n",
      "72/72 [==============================] - 0s 496us/step - loss: 0.6781 - acc: 0.7917\n",
      "Epoch 4/5\n",
      "72/72 [==============================] - 0s 486us/step - loss: 0.6680 - acc: 0.7917\n",
      "Epoch 5/5\n",
      "72/72 [==============================] - 0s 483us/step - loss: 0.6577 - acc: 0.7778\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TextNeuralNetPipeline(factory_class=None, num_labels=2,\n",
       "           text2seq=Text2Sequence(pad_string='', stemmer=<FrenchStemmer>,\n",
       "       tokenizer=<nltk.tokenize.treebank.TreebankWordTokenizer object at 0x1a261b17f0>))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, \n",
    "                        epochs=5,\n",
    "                        batch_size=100,\n",
    "                        shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "average = 'binary'\n",
    "precision_score = metrics.precision_score(y_test.astype('int'), y_pred, average=average)\n",
    "recall_score = metrics.recall_score(y_test.astype('int'), y_pred, average=average)\n",
    "f_score = metrics.f1_score(y_test.astype('int'), y_pred, average=average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Precision : \" + str(precision_score))\n",
    "print(\"Recall : \" + str(recall_score))\n",
    "print(\"F-Score : \" + str(f_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_proba = model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fpr, tpr, _ = metrics.roc_curve(y_test.astype('int'), y_pred_proba[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lw=1\n",
    "plt.plot(fpr, tpr, color='darkorange',lw=lw)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic (ROC)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avoir un pipeline complet est très pratique, cependant, on peut avoir besoin de redécomposer ce pipeline pour le découper en plus petits traitemtents (DAG Airflow) ou pour effectuer de la cross validation. Ici, on ne va transformer le texte en séquences qu'une seule fois et c'est le modèle qu'on va lancer plusieurs fois."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extrait du code de la méthode `fit` de la classe TextNeuralNetPipeline:\n",
    "\n",
    "```python\n",
    "x = self.text2seq.fit_transform(X)\n",
    "y_enc = np_utils.to_categorical(y, self.num_labels)\n",
    "\n",
    "self.model_ = KerasClassifier(build_fn=self.factory.create_model,\n",
    "                              dictionary_size=self.text2seq.dictionary_size_,\n",
    "                              num_labels=self.num_labels)\n",
    "\n",
    "self.model_.fit(x, y_enc, **kwargs)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il suffit de décomposer le traitement, avec d'un côté un objet `Text2Sequence` et de l'autre un `KerasClassifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bdacore.neural_networks.processing import Text2Sequence\n",
    "\n",
    "text2seq = Text2Sequence()\n",
    "x = text2seq.fit_transform(X['text'])\n",
    "y = X['class_cat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "clf_keras = KerasClassifier(build_fn=LSTMFactory().create_model,\n",
    "                              dictionary_size=text2seq.dictionary_size_,\n",
    "                              num_labels=2,\n",
    "                              epochs=5,\n",
    "                              batch_size=100, \n",
    "                              verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On est maintenant en mesure d'effectuer la cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "cv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "scores = cross_val_score(clf_keras, x.astype(\"int\"), y.astype(\"int\"), cv=cv, scoring='roc_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(scores)\n",
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le modèle est plutôt stable sur l'AUC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Debrief:\n",
    "Avec une approche text2seq assez simple, et un pipeline à base de LSTM, on obtient, avec le packaging proposé, des résultats rapides, le tout *sklearn compliant*, permettant tout un tas de joyeusetés, type cross validation et j'en passe, le tout avec une performance au niveau des métriques assez bonne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bdacore_py3",
   "language": "python",
   "name": "bdacore_py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
